import Makie
import LaTeXStrings
import CairoMakie
import Flux
import LinearAlgebra
import StaticArrays


function create_batch_signals_full_data(batch_size_create_data::Int, listening_length::Int; mic_rate::Int=44000, dt::Float64=1/mic_rate)
    rand_float_0_1 = rand(Float64,3*batch_size_create_data)
    results = zeros(batch_size_create_data,listening_length)
    for index in range(0,batch_size_create_data-1)
        results[index+1,:] = rand_float_0_1[3*index+1]*sin.(rand_float_0_1[3*index+2]*20_000* dt *(0:listening_length-1) .+ rand_float_0_1[3*index+3])
    end
    return results
end


function faster_prepare_data_full_learn(data;
    ear_positions=StaticArrays.SVector{3,Float64}.((1,0,0,0),(0,1,0,0),(0,0,1,0)),
    speed_sound = 343.,
    mic_rate::Int=44000,
    dt::Float64=1/mic_rate,
    num_ears::Int=length(ear_positions))    
    
    batch_size, listening_length = size(data)
    positions_sound = rand(3*batch_size)*200 .- 100

    distances = [[LinearAlgebra.norm(positions_sound[3*index+1:3*index+3] .- ear_positions[n_ear]) for n_ear in 1:num_ears] for index in range(0,batch_size-1)]
    times = [ceil.(Int,(dist/speed_sound .- minimum(dist/speed_sound))/dt) for dist in distances]
    
    results = [[Vector{Float64}(undef, listening_length) for _ in 1:num_ears] for _ in 1:batch_size]
    for index in range(1,batch_size)
        row = @view data[index, :]
        for n_ear in 1:num_ears
            circshift!(results[index][n_ear],row,times[index][n_ear])
            results[index][n_ear] .*= (1/distances[index][n_ear]^2)
            results[index][n_ear][1:times[index][n_ear]] .= 0
        end
    end
    return results,times,distances
end

function flat_prepare_data_full_learn(data;
    speed_sound = 343.,
    mic_rate::Int=44000,
    dt::Float64=1/mic_rate,
    )    
    
    batch_size, listening_length = size(data)
    positions_sound = rand(3*batch_size)*20 .- 10

    # distances = Vector{Float64}(undef, batch_size)
    # times = similar(distances)

    disttime = Vector{Float64}(undef, batch_size*2)
    results = [Vector{Float64}(undef, listening_length) for _ in 1:batch_size]
    for index in range(1,batch_size)
        row = @view data[index, :]
        # distances[index] = LinearAlgebra.norm(positions_sound[3*index+1:3*index+3])
        # times[index] = ceil(Int,(distances[index]/speed_sound)/dt)
        disttime[2*index-1] = LinearAlgebra.norm(positions_sound[3*(index-1)+1:3*(index-1)+3])
        disttime[2*index] = ceil(Int,(disttime[2*index-1]/speed_sound)/dt)

        circshift!(results[index],row,disttime[2*index])
        results[index] .*= (1/disttime[2*index-1]^2)
        results[index][1:Int(disttime[2*index])] .= 0
    end
    return results,disttime
end


function prepare_for_single_signal_CNN(data_learn,times,distances,batch_size,listening_length)
    signals = collect(Base.Iterators.flatten(data_learn))
    X = reshape(collect(Base.Iterators.flatten(signals)),(1,listening_length,1,4*batch_size))
    
    times = collect(Base.Iterators.flatten(times))
    amplitudes = 1 ./ collect(Base.Iterators.flatten(distances)) .^2
    Y = [times amplitudes]
    return X,Y
end


function do_ki_CNN(batch_size_create_data,listening_length,batches_per_epoch,num_ears;epochs=2,new_data = 5, print_every = batches_per_epoch//100, evaluation_batch_size = batch_size_create_data)
    modelCNN = Flux.Chain(
    # First convolution, operating upon a listeninglengthxnum_ears image
    Flux.Conv((1, 200), 1=>16, pad=Flux.SamePad(), Flux.leakyrelu),
    Flux.MaxPool((1,3)),
    # x -> Flux.maxpool(x, (1,3)),

    # Second convolution, operating upon a ...x... image
    Flux.Conv((1, 3), 16=>32, pad=Flux.SamePad(), Flux.leakyrelu),
    Flux.MaxPool((1,3)),
    # x -> Flux.maxpool(x, (1,3)),

    # Third convolution, operating upon a ...x... image
    Flux.Conv((1, 3), 32=>32, pad=Flux.SamePad(), Flux.leakyrelu),
    Flux.MaxPool((1,3)),
    # x -> Flux.maxpool(x, (1,3)),

    # Reshape 3d tensor into a 2d one, at this point it should be (3, 3, 32, N)
    # which is where we get the 288 in the `Dense` layer below:
    Flux.flatten,
    # x -> Flux.reshape(x, :, size(x, 4)),
    Flux.Dense(Int(floor(listening_length/27)*32), 2),


    # Flux.tanhshrink,
    Flux.leakyrelu,

    )
    model = Flux.f64(modelCNN)

    opt_state = Flux.setup(Flux.Adam(), model)
    # opt_state = Flux.setup(Flux.NADAM(), model)
    # opt_state = Flux.setup(Flux.Descent(), model)
    # opt_state = Flux.setup(Flux.Momentum(), model)


    train_accuracies = zeros(epochs)
    test_accuracies = zeros(epochs)

    X_train,times_train,distances_train = faster_prepare_data_full_learn(create_batch_signals_full_data(batch_size_create_data*batches_per_epoch,listening_length))
    X_train,Y_train = prepare_for_single_signal_CNN(X_train, times_train, distances_train, batch_size_create_data*batches_per_epoch, listening_length)

    X_test,times_test,distances_test = faster_prepare_data_full_learn(create_batch_signals_full_data(evaluation_batch_size,listening_length))
    X_test,Y_test = prepare_for_single_signal_CNN(X_test,times_test,distances_test, evaluation_batch_size, listening_length)
    # (1, listening_length, 1, batch_size*4)
    #println("size of position array: $(size(Y_test))")
    #println("size of data array: $(size(X_train))")


    for epoch in 1:epochs

        if epoch%new_data==0
            X_train,times_train,distances_train = faster_prepare_data_full_learn(create_batch_signals_full_data(batch_size_create_data*batches_per_epoch,listening_length))
            X_train,Y_train = prepare_for_single_signal_CNN(X_train, times_train, distances_train, batch_size_create_data*batches_per_epoch, listening_length)
        end
        
        model = Flux.trainmode!(model)


        for index in 1:batches_per_epoch
            batch_start = (index-1) * batch_size_create_data + 1
            batch_end = index * batch_size_create_data
            # Calculate the gradient of the objective
            # with respect to the parameters within the model:
            grads = Flux.gradient(model) do m
                result = m(X_train[:,:,:,batch_start:batch_end])
                Flux.Losses.mse(result, Y_train[batch_start:batch_end,:]') # +sum(sum.(abs2,Flux.trainables(m)))    # L1 pruning now slower
            end
            # Update the parameters so as to reduce the objective,
            # according the chosen optimisation rule:
            Flux.update!(opt_state, model, grads[1])
            #print(grads)
            #return
            if index%print_every==0
                print("\r")
                print("Epoch: $epoch Advanced: $(round(index/batches_per_epoch*100,digits=1))%")
            end
        end
        # opt_state = Flux.setup(Flux.Adam(10/(2*epoch)), model)

        g_mse_train = 0.
        g_mse_test = 0.
        for index in 1:evaluation_batch_size
            g_mse_train +=Flux.Losses.mse(model(X_train[:,:,:,index:index]),Y_train[index,:])/evaluation_batch_size
            g_mse_test +=Flux.Losses.mse(model(X_test[:,:,:,index:index]),Y_test[index,:])/evaluation_batch_size
        end
        train_accuracies[epoch] = sqrt(g_mse_train)
        test_accuracies[epoch] = sqrt(g_mse_test)
        print("\r")
        println("Epoch: $epoch , g_mse_train: $(sqrt(g_mse_train)) , g_mse_test: $(sqrt(g_mse_test))")
    end

    #@time data_test,positions_test = less_d_faster_prepare_data_full_learn(create_batch_signals_full_data(batch_size_create_data,listening_length))

    g_mse = 0.
    for index in 1:batch_size_create_data
        g_mse +=Flux.Losses.mse(model(X_train[:,:,:,index:index]),Y_train[index,:])/batch_size_create_data
    end
    println()
    println("the average mse is: $(sqrt(g_mse))")

    println("the supposed time, amplitude is: $(Y_train[1,:]) the prediction is: $(model(X_train[:,:,:,1:1]))")
    println("ratios: $(model(X_train[:,:,:,1:1]) ./ Y_train[1,:])")
    
    # println("mse of model output is: $(Flux.Losses.mae(model(X_train[:,:,:,1:1]),positions[1:3]))")
    
    return train_accuracies,test_accuracies
end


function flat_do_ki_CNN(batch_size_create_data,listening_length,batches_per_epoch,num_ears;epochs=2,new_data = 5, print_every = batches_per_epoch//100, evaluation_batch_size = batch_size_create_data)
    modelCNN = Flux.Chain(
    # First convolution, operating upon a listeninglengthxnum_ears image
    Flux.Conv((1,200), 1=>16, pad=Flux.SamePad(), Flux.leakyrelu),
    Flux.MaxPool((1,3)),
    # x -> Flux.maxpool(x, (1,3)),

    # Second convolution, operating upon a ...x... image
    Flux.Conv((1,3), 16=>32, pad=Flux.SamePad(), Flux.leakyrelu),
    Flux.MaxPool((1,3)),
    # x -> Flux.maxpool(x, (1,3)),

    # Third convolution, operating upon a ...x... image
    Flux.Conv((1,3), 32=>32, pad=Flux.SamePad(), Flux.leakyrelu),
    Flux.MaxPool((1,3)),
    # x -> Flux.maxpool(x, (1,3)),

    # Reshape 3d tensor into a 2d one, at this point it should be (3, 3, 32, N)
    # which is where we get the 288 in the `Dense` layer below:
    Flux.flatten,
    # x -> Flux.reshape(x, :, size(x, 4)),
    Flux.Dense(Int(floor(listening_length/27)*32), 2),


    # Flux.tanhshrink,
    Flux.leakyrelu,

    )
    model = Flux.f64(modelCNN)

    opt_state = Flux.setup(Flux.NADAM(), model)
    # opt_state = Flux.setup(Flux.Descent(), model)
    # opt_state = Flux.setup(Flux.Momentum(), model)


    train_accuracies = zeros(epochs)
    test_accuracies = zeros(epochs)

    X_train,Y_train = flat_prepare_data_full_learn(create_batch_signals_full_data(batch_size_create_data*batches_per_epoch,listening_length))
    # X_train,Y_train = prepare_for_single_signal_CNN(X_train, times_train, distances_train, batch_size_create_data*batches_per_epoch, listening_length)

    X_test,Y_test = flat_prepare_data_full_learn(create_batch_signals_full_data(evaluation_batch_size,listening_length))
    # X_test,Y_test = prepare_for_single_signal_CNN(X_test,times_test,distances_test, evaluation_batch_size, listening_length)
    # (1, listening_length, 1, batch_size*4)
    #println("size of position array: $(size(Y_test))")
    #println("size of data array: $(size(X_train))")


    for epoch in 1:epochs

        if epoch%new_data==0
            X_train,Y_train = flat_prepare_data_full_learn(create_batch_signals_full_data(batch_size_create_data*batches_per_epoch,listening_length))
            # X_train,Y_train = prepare_for_single_signal_CNN(X_train, times_train, distances_train, batch_size_create_data*batches_per_epoch, listening_length)
        end
        
        model = Flux.trainmode!(model)


        for index in 1:batches_per_epoch
            batch_start = (index-1) * batch_size_create_data + 1
            batch_end = index * batch_size_create_data
            rawX_batch = X_train[batch_start:batch_end]
            X_batch = reshape(reduce(hcat, rawX_batch), 1, listening_length, 1, batch_size_create_data)
            rawY_batch = Y_train[2*batch_start-1:2*batch_end]
            Y_batch = reshape(rawY_batch, 2, :)

            # Calculate the gradient of the objective
            # with respect to the parameters within the model:
            grads = Flux.gradient(model) do m
                result = m(X_batch)
                Flux.Losses.mse(result, Y_batch) # +sum(sum.(abs2,Flux.trainables(m)))    # L1 pruning now slower
            end
            # Update the parameters so as to reduce the objective,
            # according the chosen optimisation rule:
            Flux.update!(opt_state, model, grads[1])
            #print(grads)
            #return
            if index%print_every==0
                print("\r")
                print("Epoch: $epoch Advanced: $(round(index/batches_per_epoch*100,digits=1))%")
            end
        end
        # opt_state = Flux.setup(Flux.Adam(10/(2*epoch)), model)

        g_mse_train = 0.
        g_mse_test = 0.
        for index in 1:evaluation_batch_size
            g_mse_train +=Flux.Losses.mse(model(reshape(X_train[index],1,listening_length,1,1)),Y_train[2*index-1:2*index])/evaluation_batch_size
            g_mse_test +=Flux.Losses.mse(model(reshape(X_test[index],1,listening_length,1,1)),Y_test[2*index-1:2*index])/evaluation_batch_size
        end
        train_accuracies[epoch] = sqrt(g_mse_train)
        test_accuracies[epoch] = sqrt(g_mse_test)
        print("\r")
        println("Epoch: $epoch , g_mse_train: $(sqrt(g_mse_train)) , g_mse_test: $(sqrt(g_mse_test))")
    end

    #@time data_test,positions_test = less_d_faster_prepare_data_full_learn(create_batch_signals_full_data(batch_size_create_data,listening_length))

    g_mse = 0.
    for index in 1:batch_size_create_data
        g_mse +=Flux.Losses.mse(model(reshape(X_train[index],1,listening_length,1,1)),Y_train[2*index-1:2*index])/batch_size_create_data
    end
    println()
    println("the average mse is: $(sqrt(g_mse))")

    println("the supposed time, amplitude is: $(Y_train[1:2]) the prediction is: $(model(X_train[1]))")
    
    # println("mse of model output is: $(Flux.Losses.mae(model(X_train[:,:,:,1:1]),positions[1:3]))")
    
    return train_accuracies,test_accuracies
end

saving_plot_path = (@__DIR__)*"/plots/"
saving_data_path = (@__DIR__)*"/data/"
saving_data_to = saving_data_path*"train_batch.txt"

listening_length = 4400
batch_size_create_data = 100
batches_per_epoch = 10

@time train_acc,test_acc = flat_do_ki_CNN(batch_size_create_data,listening_length,batches_per_epoch,4;epochs=50,new_data=2,print_every=1,evaluation_batch_size=5)


function plot_single_ear_data(data_learn;num_ears=4)
    fig = Makie.Figure()
    ax = Makie.Axis(fig[1, 1],title = "many signals",
    xlabel = LaTeXStrings.LaTeXString("time/dt"),
    ylabel = LaTeXStrings.LaTeXString("signal(t)"))
    for index_ear in range(1,num_ears)
        Makie.lines!(ax, data_learn[1][index_ear],label="signal ear: $index_ear")
    end
    # Makie.lines!(collect(Base.Iterators.flatten(data_learn))[1],label="test")
    Makie.axislegend()
    CairoMakie.display(fig)
    # CairoMakie.save(saving_plot_path*"Test_single_ear.png",fig)
end

function plot_accuracy(train_acc,test_acc,plot_name)
    fig = Makie.Figure()
    ax = Makie.Axis(fig[1, 1],title = "loss over epochs",
    xlabel = LaTeXStrings.LaTeXString("epochs"),
    ylabel = LaTeXStrings.LaTeXString("loss"))
    Makie.lines!(ax,1:length(train_acc), train_acc,label="train")
    Makie.lines!(ax, 1:length(train_acc),test_acc,label="test")

    Makie.axislegend()
    CairoMakie.display(fig)
    CairoMakie.save(saving_plot_path*plot_name*".png",fig)
end


plot_accuracy(train_acc,test_acc,"first_cnn_test_single_signal")